{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3750a8-791b-4bd8-a887-2684fc4b3a43",
   "metadata": {},
   "source": [
    "# Software Engineering Study Assistant - RAG Pipeline\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) chatbot designed to help software engineering students with:\n",
    "- **Understanding complex topics** from lecture notes and textbooks\n",
    "- **Solving previous year exam questions** with detailed explanations\n",
    "- **Getting contextual answers** from course materials and PDFs\n",
    "- **Study assistance** with proper references to source materials\n",
    "\n",
    "**Technology Stack:**\n",
    "- **PyMuPDF** for PDF lecture notes extraction\n",
    "- **LangChain's RecursiveCharacterTextSplitter** for intelligent text chunking\n",
    "- **Sentence Transformers** (all-MiniLM-L6-v2) for semantic embeddings\n",
    "- **ChromaDB** for fast similarity search across study materials\n",
    "- **LangChain's retriever** for relevant content retrieval\n",
    "- **Gemini Pro** for generating comprehensive answers with context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40ff48-d0a0-4c31-8b82-3bc1551f7801",
   "metadata": {},
   "source": [
    "## Study Assistant Pipeline Flow\n",
    "\n",
    "```\n",
    "Lecture Notes PDFs → PyMuPDF → Text Extraction → RecursiveCharacterTextSplitter → Knowledge Chunks\n",
    "                                                           ↓\n",
    "                                              Sentence Transformers → Semantic Embeddings → ChromaDB Knowledge Base\n",
    "                                                           ↓\n",
    "Student Question/Problem → Query Embedding → LangChain Retriever → Relevant Study Materials\n",
    "                                                           ↓\n",
    "                        Gemini Pro ← Context + Question → Detailed Answer with References\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- \"Explain object-oriented programming concepts\"\n",
    "- \"How do I solve this data structures problem?\"\n",
    "- \"What are the key points about software testing methodologies?\"\n",
    "- \"Help me understand this previous year question on algorithms\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa5e3c-5486-4fc6-b316-958d3bc895ac",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install all required packages using the requirements.txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2257b5fe-79df-4791-9c18-3ab2222d7330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF>=1.23.0 in ./jupyter_env/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.26.4)\n",
      "Collecting langchain>=0.1.0 (from -r requirements.txt (line 8))\n",
      "  Using cached langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community (from -r requirements.txt (line 9))\n",
      "  Using cached langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-google-genai>=1.0.0 (from -r requirements.txt (line 10))\n",
      "  Using cached langchain_google_genai-2.1.11-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sentence-transformers>=2.2.0 (from -r requirements.txt (line 13))\n",
      "  Using cached sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting chromadb>=0.4.0 (from -r requirements.txt (line 16))\n",
      "  Using cached chromadb-1.0.21-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting google-generativeai>=0.3.0 (from -r requirements.txt (line 19))\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain>=0.1.0->-r requirements.txt (line 8))\n",
      "  Using cached langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain>=0.1.0->-r requirements.txt (line 8))\n",
      "  Using cached langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./jupyter_env/lib/python3.12/site-packages (from langchain>=0.1.0->-r requirements.txt (line 8)) (0.4.28)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./jupyter_env/lib/python3.12/site-packages (from langchain>=0.1.0->-r requirements.txt (line 8)) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./jupyter_env/lib/python3.12/site-packages (from langchain>=0.1.0->-r requirements.txt (line 8)) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./jupyter_env/lib/python3.12/site-packages (from langchain>=0.1.0->-r requirements.txt (line 8)) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./jupyter_env/lib/python3.12/site-packages (from langchain>=0.1.0->-r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./jupyter_env/lib/python3.12/site-packages (from langchain-community->-r requirements.txt (line 9)) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./jupyter_env/lib/python3.12/site-packages (from langchain-community->-r requirements.txt (line 9)) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in ./jupyter_env/lib/python3.12/site-packages (from langchain-community->-r requirements.txt (line 9)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in ./jupyter_env/lib/python3.12/site-packages (from langchain-community->-r requirements.txt (line 9)) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./jupyter_env/lib/python3.12/site-packages (from langchain-community->-r requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./jupyter_env/lib/python3.12/site-packages (from langchain-community->-r requirements.txt (line 9)) (2.3.3)\n",
      "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai>=1.0.0->-r requirements.txt (line 10))\n",
      "  Using cached google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filetype<2,>=1.2 in ./jupyter_env/lib/python3.12/site-packages (from langchain-google-genai>=1.0.0->-r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./jupyter_env/lib/python3.12/site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (4.56.1)\n",
      "Requirement already satisfied: tqdm in ./jupyter_env/lib/python3.12/site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.2.0->-r requirements.txt (line 13))\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: scikit-learn in ./jupyter_env/lib/python3.12/site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (1.7.2)\n",
      "Requirement already satisfied: scipy in ./jupyter_env/lib/python3.12/site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./jupyter_env/lib/python3.12/site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (0.35.0)\n",
      "Requirement already satisfied: Pillow in ./jupyter_env/lib/python3.12/site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./jupyter_env/lib/python3.12/site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (4.15.0)\n",
      "Requirement already satisfied: build>=1.0.3 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./jupyter_env/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 16)) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (1.37.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.4.0->-r requirements.txt (line 16))\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb>=0.4.0->-r requirements.txt (line 16))\n",
      "  Using cached opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (1.75.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (0.17.4)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb>=0.4.0->-r requirements.txt (line 16))\n",
      "  Using cached kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./jupyter_env/lib/python3.12/site-packages (from chromadb>=0.4.0->-r requirements.txt (line 16)) (4.25.1)\n",
      "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-generativeai>=0.3.0 (from -r requirements.txt (line 19))\n",
      "  Using cached google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "  Using cached google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-google-genai>=1.0.0 (from -r requirements.txt (line 10))\n",
      "  Using cached langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai>=1.0.0->-r requirements.txt (line 10))\n",
      "  Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting langchain-google-genai>=1.0.0 (from -r requirements.txt (line 10))\n",
      "  Using cached langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai>=0.3.0->-r requirements.txt (line 19))\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai>=0.3.0->-r requirements.txt (line 19))\n",
      "  Using cached google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai>=0.3.0->-r requirements.txt (line 19))\n",
      "  Using cached google_api_python_client-2.181.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in ./jupyter_env/lib/python3.12/site-packages (from google-generativeai>=0.3.0->-r requirements.txt (line 19)) (2.40.3)\n",
      "Requirement already satisfied: protobuf in ./jupyter_env/lib/python3.12/site-packages (from google-generativeai>=0.3.0->-r requirements.txt (line 19)) (5.29.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./jupyter_env/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (1.26.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./jupyter_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 9)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./jupyter_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./jupyter_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 9)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./jupyter_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 9)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./jupyter_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 9)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./jupyter_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 9)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./jupyter_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 9)) (1.20.1)\n",
      "Requirement already satisfied: packaging>=19.1 in ./jupyter_env/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.4.0->-r requirements.txt (line 16)) (25.0)\n",
      "Requirement already satisfied: pyproject_hooks in ./jupyter_env/lib/python3.12/site-packages (from build>=1.0.3->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./jupyter_env/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community->-r requirements.txt (line 9)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./jupyter_env/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./jupyter_env/lib/python3.12/site-packages (from google-api-core->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./jupyter_env/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./jupyter_env/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./jupyter_env/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (4.9.1)\n",
      "Requirement already satisfied: anyio in ./jupyter_env/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./jupyter_env/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./jupyter_env/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./jupyter_env/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./jupyter_env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./jupyter_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./jupyter_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./jupyter_env/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (1.1.10)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./jupyter_env/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./jupyter_env/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./jupyter_env/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (0.27.1)\n",
      "Requirement already satisfied: six>=1.9.0 in ./jupyter_env/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./jupyter_env/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./jupyter_env/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./jupyter_env/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./jupyter_env/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./jupyter_env/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./jupyter_env/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./jupyter_env/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain>=0.1.0->-r requirements.txt (line 8)) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./jupyter_env/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./jupyter_env/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 8)) (0.25.0)\n",
      "Requirement already satisfied: coloredlogs in ./jupyter_env/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 16)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./jupyter_env/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 16)) (25.2.10)\n",
      "Requirement already satisfied: sympy in ./jupyter_env/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./jupyter_env/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in ./jupyter_env/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in ./jupyter_env/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.37.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 16))\n",
      "  Using cached opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./jupyter_env/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in ./jupyter_env/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./jupyter_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./jupyter_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 8)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./jupyter_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./jupyter_env/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./jupyter_env/lib/python3.12/site-packages (from requests<3,>=2->langchain>=0.1.0->-r requirements.txt (line 8)) (3.4.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./jupyter_env/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./jupyter_env/lib/python3.12/site-packages (from rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in ./jupyter_env/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.0->-r requirements.txt (line 8)) (3.2.4)\n",
      "Requirement already satisfied: setuptools in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (80.9.0)\n",
      "Requirement already satisfied: networkx in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./jupyter_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./jupyter_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (2025.9.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./jupyter_env/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (0.6.2)\n",
      "Requirement already satisfied: click>=8.0.0 in ./jupyter_env/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./jupyter_env/lib/python3.12/site-packages (from typer>=0.9.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./jupyter_env/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 16)) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./jupyter_env/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 16)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./jupyter_env/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./jupyter_env/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 16)) (15.0.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in ./jupyter_env/lib/python3.12/site-packages (from google-api-python-client->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (0.31.0)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai>=0.3.0->-r requirements.txt (line 19))\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./jupyter_env/lib/python3.12/site-packages (from google-api-python-client->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (4.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./jupyter_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./jupyter_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in ./jupyter_env/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (1.71.2)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in ./jupyter_env/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (3.2.4)\n",
      "Requirement already satisfied: zipp>=3.20 in ./jupyter_env/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./jupyter_env/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain>=0.1.0->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./jupyter_env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./jupyter_env/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.3.0->-r requirements.txt (line 19)) (0.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./jupyter_env/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./jupyter_env/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./jupyter_env/lib/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 16)) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./jupyter_env/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 16)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./jupyter_env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (3.0.2)\n",
      "Using cached langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
      "Using cached langchain_google_genai-2.0.10-py3-none-any.whl (41 kB)\n",
      "Using cached google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Using cached sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Using cached chromadb-1.0.21-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.8 MB)\n",
      "Using cached google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Using cached kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "Using cached langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
      "Using cached opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n",
      "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "Using cached google_api_python_client-2.181.0-py3-none-any.whl (14.1 MB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: torch, opentelemetry-semantic-conventions, kubernetes, google-auth-httplib2, google-api-core, opentelemetry-sdk, langchain-core, google-api-python-client, sentence-transformers, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, google-ai-generativelanguage, langchain, google-generativeai, chromadb, langchain-google-genai, langchain-community\n",
      "Successfully installed chromadb-1.0.21 google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.181.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 kubernetes-33.1.0 langchain-0.3.27 langchain-community-0.3.29 langchain-core-0.3.76 langchain-google-genai-2.0.10 langchain-text-splitters-0.3.11 opentelemetry-exporter-otlp-proto-grpc-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 sentence-transformers-5.1.0 torch-2.8.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for RAG pipeline using requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c15eba-f97e-4555-b3d8-5607ad5dbcb5",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a703d29-9cb8-4040-8296-d5d7fd86090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.schema import BaseMessage, HumanMessage, AIMessage\n",
    "import uuid\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0dde4d-af1a-4f7e-bb42-6435e1a1ac60",
   "metadata": {},
   "source": [
    "## Gemini API Key Setup\n",
    "\n",
    "Get your free Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6deb0-ff3c-4efa-a2c5-b2585902df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Gemini API key configured\n",
      "API key starts with: AIzaSyCA...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Gemini API key from environment variable\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Verify API key is set\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"⚠️  Please set your Gemini API key!\")\n",
    "    print(\"1. Create a .env file in this directory\")\n",
    "    print(\"2. Add this line: GEMINI_API_KEY=your_actual_api_key_here\")\n",
    "    print(\"3. Or use: cp .env.example .env and edit it\")\n",
    "    print(\"Get your free API key from: https://makersuite.google.com/app/apikey\")\n",
    "else:\n",
    "    print(\"✅ Gemini API key loaded from .env file\")\n",
    "    print(f\"API key starts with: {GEMINI_API_KEY[:8]}...\")\n",
    "\n",
    "# Set environment variable for Google Generative AI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1bf069-b68b-4778-9986-c735eadc6ba0",
   "metadata": {},
   "source": [
    "## PDF Text Extraction with PyMuPDF\n",
    "\n",
    "This section handles text-based PDFs using direct text extraction:\n",
    "\n",
    "- **Text-based PDFs**: Direct text extraction using PyMuPDF for PDFs created from digital documents (Word, LaTeX, Google Docs, etc.)\n",
    "\n",
    "**Supported PDF Types:**\n",
    "- Documents created from Word processors\n",
    "- LaTeX-generated PDFs  \n",
    "- Google Docs exports\n",
    "- Any PDF with embedded text data\n",
    "\n",
    "The pipeline uses PyMuPDF for fast and accurate text extraction from digital documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "969fd99f-fd8c-48af-a86f-6d0681f972be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Text-based PDFs ===\n",
      "Processing: metrics3.pdf\n",
      "  → Using direct text extraction\n",
      "Extracted 17980 characters from metrics3.pdf\n",
      "Processing: Lecture#7.pdf\n",
      "  → Using direct text extraction\n",
      "Extracted 19743 characters from Lecture#7.pdf\n",
      "Processing: Sample.pdf\n",
      "  → Using direct text extraction\n",
      "Extracted 20737 characters from Sample.pdf\n",
      "Processing: GreedyAlgorithms.pdf\n",
      "  → Using direct text extraction\n",
      "Extracted 19614 characters from GreedyAlgorithms.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if all_extracted_text:\\n    print(f\"\\n=== EXTRACTION SUMMARY ===\")\\n    print(f\"Total extracted content: {len(all_extracted_text)} characters\")\\n    print(f\"PDFs processed: {len([p for p in pdf_paths if os.path.exists(p)])}\")\\n    print(f\"First 500 characters:\\n{all_extracted_text[:500]}...\")\\nelse:\\n    print(\"\\nNo PDF files were processed.\")\\n    print(\"Please add your text-based PDFs to the pdf_paths list\")'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from text-based PDFs using PyMuPDF\n",
    "    Use this for PDFs created from digital documents (Word, LaTeX, Google Docs, etc.)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Warning: PDF file not found: {pdf_path}\")\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"Processing: {os.path.basename(pdf_path)}\")\n",
    "    print(f\"  → Using direct text extraction\")\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        page_text = page.get_text()\n",
    "        \n",
    "        if page_text.strip():  # Only add non-empty pages\n",
    "            text += f\"\\n\\n--- Lecture Page {page_num + 1} ---\\n\\n\"\n",
    "            text += page_text\n",
    "    \n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "# Add your text-based PDFs here (created from Word, LaTeX, Google Docs, etc.)\n",
    "pdf_paths = [\n",
    "    \"./assets/metrics3.pdf\", \n",
    "    \"./assets/Lecture#7.pdf\",\n",
    "    \"./assets/Sample.pdf\",\n",
    "    \"./assets/GreedyAlgorithms.pdf\"\n",
    "]\n",
    "\n",
    "extracted_texts = {}\n",
    "\n",
    "print(\"=== Processing Text-based PDFs ===\")\n",
    "for pdf_path in pdf_paths:\n",
    "    if os.path.exists(pdf_path):\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        extracted_texts[os.path.basename(pdf_path)] = text\n",
    "        print(f\"Extracted {len(text)} characters from {os.path.basename(pdf_path)}\")\n",
    "    else:\n",
    "        print(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "\"\"\"if all_extracted_text:\n",
    "    print(f\"\\n=== EXTRACTION SUMMARY ===\")\n",
    "    print(f\"Total extracted content: {len(all_extracted_text)} characters\")\n",
    "    print(f\"PDFs processed: {len([p for p in pdf_paths if os.path.exists(p)])}\")\n",
    "    print(f\"First 500 characters:\\n{all_extracted_text[:500]}...\")\n",
    "else:\n",
    "    print(\"\\nNo PDF files were processed.\")\n",
    "    print(\"Please add your text-based PDFs to the pdf_paths list\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64396d45-24a6-4f74-bd6f-837192ce30aa",
   "metadata": {},
   "source": [
    "### Understanding Text-based PDF Processing\n",
    "\n",
    "**Text-based PDFs**: Created from digital documents (Word, LaTeX, Google Docs, etc.) - contain actual text data that can be directly extracted.\n",
    "\n",
    "**Processing Features:**\n",
    "- Fast direct text extraction using PyMuPDF\n",
    "- Maintains original text formatting and structure\n",
    "- Works with all standard PDF formats containing embedded text\n",
    "- Preserves page structure with clear page separators\n",
    "\n",
    "**Best Results With:**\n",
    "- Documents created from word processors (Word, Google Docs, etc.)\n",
    "- LaTeX-generated academic papers and textbooks\n",
    "- Exported PDFs from presentation software\n",
    "- Any PDF with selectable/copyable text\n",
    "\n",
    "**Note**: This pipeline is optimized for text-based PDFs. If you have scanned documents (images of text), you would need OCR functionality, which can be added later if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaee036-7fe3-428b-9eae-43b3295a9222",
   "metadata": {},
   "source": [
    "## Text Chunking with LangChain's RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c35b93c7-bdef-4e5f-8712-e8f08fd5d6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks with Unknown source: 0\n"
     ]
    }
   ],
   "source": [
    "def create_overlapping_chunks(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split lecture notes into overlapping chunks for better retrieval\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"chunk_id\": i,\n",
    "                \"source\": \"Unknown\",  # default, will be overwritten later\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"content_type\": \"lecture_notes\"\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "documents = []\n",
    "\n",
    "for source, text in extracted_texts.items():\n",
    "    docs = create_overlapping_chunks(text)\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"source\"] = source\n",
    "    documents.extend(docs)\n",
    "\n",
    "unknown_sources = [doc for doc in documents if doc.metadata.get(\"source\") == \"Unknown\"]\n",
    "print(f\"Chunks with Unknown source: {len(unknown_sources)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570165e3-f318-4750-9b7c-dff533916910",
   "metadata": {},
   "source": [
    "## Initialize Sentence Transformers for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e03069e3-86c4-41b0-b386-a42c1f63ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Transformers (all-MiniLM-L6-v2) model loaded\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Initialize Sentence Transformers embeddings\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Sentence Transformers (all-MiniLM-L6-v2) model loaded\")\n",
    "print(f\"Embedding dimension: 384\")  # all-MiniLM-L6-v2 produces 384-dimensional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3738d9-c27b-468a-873d-d1bd54c7d643",
   "metadata": {},
   "source": [
    "## Create ChromaDB Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "574200c9-7d97-480e-9fb0-692f629b337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineering Knowledge Base created with 124 chunks\n",
      "Database persisted to: ./chroma_db\n",
      "Your study materials are now ready for questions!\n",
      "Loaded vectorstore document metadata samples:\n",
      "Document 1 metadata: {'source': 'Lecture#7.pdf', 'content_type': 'lecture_notes', 'chunk_size': 900, 'chunk_id': 11}\n",
      "Content preview: Cost Estimation Process\n",
      "Errors\n",
      "Effort\n",
      "Development Time\n",
      "Size Table\n",
      "Lines of Code\n",
      "Number of Use Case\n",
      "Function Point\n",
      "Estimation Process\n",
      "Number of Personn...\n",
      "\n",
      "Document 2 metadata: {'chunk_size': 900, 'source': 'Lecture#7.pdf', 'chunk_id': 11, 'content_type': 'lecture_notes'}\n",
      "Content preview: Cost Estimation Process\n",
      "Errors\n",
      "Effort\n",
      "Development Time\n",
      "Size Table\n",
      "Lines of Code\n",
      "Number of Use Case\n",
      "Function Point\n",
      "Estimation Process\n",
      "Number of Personn...\n",
      "\n",
      "Document 3 metadata: {'chunk_id': 11, 'content_type': 'lecture_notes', 'source': 'Lecture#7.pdf', 'chunk_size': 900}\n",
      "Content preview: Cost Estimation Process\n",
      "Errors\n",
      "Effort\n",
      "Development Time\n",
      "Size Table\n",
      "Lines of Code\n",
      "Number of Use Case\n",
      "Function Point\n",
      "Estimation Process\n",
      "Number of Personn...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up ChromaDB knowledge base for study materials\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Create or load ChromaDB vector store for educational content\n",
    "if 'documents' in locals() and documents:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=\"software_engineering_knowledge_base\"\n",
    "    )\n",
    "    \n",
    "    # Persist the database\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    print(f\"Software Engineering Knowledge Base created with {len(documents)} chunks\")\n",
    "    print(f\"Database persisted to: {persist_directory}\")\n",
    "    print(\"Your study materials are now ready for questions!\")\n",
    "else:\n",
    "    print(\"No study materials available for knowledge base creation\")\n",
    "print(\"Loaded vectorstore document metadata samples:\")\n",
    "results = vectorstore.similarity_search(\"test\", k=3)  # just a quick search to get some docs\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Document {i} metadata: {doc.metadata}\")\n",
    "    print(f\"Content preview: {doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc589e7-190f-4625-bf6c-a048c8ae49e0",
   "metadata": {},
   "source": [
    "## Create LangChain Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "844c0f3d-79dc-4a00-890e-98029763e424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study Materials Retriever created\n",
      "Search type: similarity\n",
      "Number of chunks retrieved per query: 5\n",
      "\n",
      "Test retrieval for 'What is Function point?':\n",
      "Retrieved 0 relevant study materials\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever for study materials\n",
    "if 'vectorstore' in locals():\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"score_threshold\": 0.5, #cosine_distance = 1 — cosine_similarity\n",
    "            \"k\": 5\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Study Materials Retriever created\")\n",
    "    print(\"Search type: similarity\")\n",
    "    print(\"Number of chunks retrieved per query: 5\")\n",
    "    \n",
    "    # Test the retriever with a typical student question\n",
    "    test_query = \"What is Function point?\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(test_query)\n",
    "    print(f\"\\nTest retrieval for '{test_query}':\")\n",
    "    print(f\"Retrieved {len(retrieved_docs)} relevant study materials\")\n",
    "    if retrieved_docs:\n",
    "        print(f\"First retrieved content preview:\\n{retrieved_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Source: {retrieved_docs[0].metadata.get('source', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"Knowledge base not available for retriever creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd0a0f",
   "metadata": {},
   "source": [
    "## Initialize Gemini Pro with API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b93a2499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Pro LLM initialized with API key\n",
      "Model: gemini-1.5-flash\n",
      "Temperature: 0.3\n",
      "Max output tokens: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758042291.522171    9078 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini Pro LLM with API key\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        google_api_key=GEMINI_API_KEY,\n",
    "        temperature=0.3,\n",
    "        max_output_tokens=1024\n",
    "    )\n",
    "    \n",
    "    print(\"Gemini Pro LLM initialized with API key\")\n",
    "    print(f\"Model: gemini-1.5-flash\")\n",
    "    print(f\"Temperature: 0.3\")\n",
    "    print(f\"Max output tokens: 1024\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini Pro: {e}\")\n",
    "    print(\"Please ensure you have:\")\n",
    "    print(\"1. Valid Gemini API key\")\n",
    "    print(\"2. Correct API key format\")\n",
    "    print(\"3. Get your key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a6cfa",
   "metadata": {},
   "source": [
    "## Create Conversational RAG Chain with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af54bcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational Software Engineering Study Assistant created successfully\n",
      "Memory: Remembers last 5 question-answer exchanges\n",
      "Chain type: Conversational Retrieval\n",
      "Returns source documents: Yes\n",
      "Ready for conversational study sessions!\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation memory to remember chat history\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,  # Remember last 5 exchanges\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Define a custom prompt template for conversational educational assistance\n",
    "qa_prompt_template = \"\"\"\n",
    "You are an AI Study Assistant for Software Engineering students. Your role is to help students understand concepts, solve problems, and prepare for exams using their course materials.\n",
    "\n",
    "Given the following conversation history and a follow-up question, provide a helpful response using the context from study materials.\n",
    "\n",
    "Instructions:\n",
    "- Consider the conversation history for better context\n",
    "- Provide clear, detailed explanations suitable for students\n",
    "- Include examples when helpful for understanding\n",
    "- Reference the source materials when possible\n",
    "- For previous year questions, provide step-by-step solutions\n",
    "- If you need to make assumptions, state them clearly\n",
    "- If the context doesn't contain enough information, say so and suggest what additional materials might help\n",
    "- Build upon previous responses when relevant\n",
    "\n",
    "Context from Study Materials:\n",
    "{context}\n",
    "\n",
    "Follow-up Question: {question}\n",
    "Study Assistant Response:\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=qa_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the Conversational Study Assistant RAG chain\n",
    "if 'llm' in locals() and 'retriever' in locals():\n",
    "    rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        return_source_documents=True,\n",
    "        combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(\"Conversational Software Engineering Study Assistant created successfully\")\n",
    "    print(\"Memory: Remembers last 5 question-answer exchanges\")\n",
    "    print(\"Chain type: Conversational Retrieval\")\n",
    "    print(\"Returns source documents: Yes\")\n",
    "    print(\"Ready for conversational study sessions!\")\n",
    "else:\n",
    "    print(\"Cannot create Study Assistant - missing LLM or retriever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4746e52",
   "metadata": {},
   "source": [
    "## Test the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42d8e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Question: What's semi-detached mode of COCOMO? what formuala are used to calculate effort in this mode?\n",
      "\n",
      "Study Assistant Answer:\n",
      "The provided text only gives an example calculation using the Basic COCOMO model in what it refers to as \"semi-detached mode,\" but it doesn't define what \"semi-detached mode\" actually means within the context of COCOMO.  The text focuses solely on the calculation itself.  Therefore, I cannot answer your question about what semi-detached mode is.\n",
      "\n",
      "To understand what \"semi-detached mode\" signifies in the COCOMO model, you need to consult additional resources, such as:\n",
      "\n",
      "* **The original COCOMO documentation:**  Look for the publication that originally defined the COCOMO model.  This will provide the precise definition of different modes (organic, semi-detached, embedded) and their implications.\n",
      "* **Your course textbook or lecture slides:**  There's likely a more complete explanation of COCOMO modes beyond the single example provided in the excerpt.  Check for sections specifically covering COCOMO model types and their characteristics.\n",
      "* **Online resources:** Search for reliable explanations of the COCOMO model, focusing on the different development modes.  Reputable software engineering websites or academic resources will offer detailed descriptions.\n",
      "\n",
      "\n",
      "The formula used to calculate effort (E) in the *example* provided is:\n",
      "\n",
      "**E = a * (KLOC)^b**\n",
      "\n",
      "Where:\n",
      "\n",
      "* **E** is the effort in person-months (PM).\n",
      "* **a** is a constant (3.0 in the example, this is specific to the semi-detached mode, but the provided text doesn't explain why).\n",
      "* **KLOC** is the thousands of lines of code.\n",
      "* **b** is an exponent (1.12 in the example, again specific to the semi-detached mode, but the reason is not explained in the provided text).\n",
      "\n",
      "Remember that this formula is only valid for the *specific example* given.  The values of 'a' and 'b' are likely to change depending on the actual COCOMO mode being used (organic, semi-detached, embedded).  Finding the definition of \"semi-detached mode\" will clarify the appropriate values for 'a' and 'b' in that context.\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: Lecture#7.pdf (Chunk 22)\n",
      "Content Preview: --- Lecture Page 48 ---\n",
      "\n",
      "Basic COCOMO model\n",
      "• Example – Consider a software project using semi-detached mode \n",
      "with 300 Kloc .find out effort estimatio...\n",
      "\n",
      "Source 2: Lecture#7.pdf (Chunk 22)\n",
      "Content Preview: --- Lecture Page 48 ---\n",
      "\n",
      "Basic COCOMO model\n",
      "• Example – Consider a software project using semi-detached mode \n",
      "with 300 Kloc .find out effort estimatio...\n",
      "\n",
      "Source 3: Lecture#7.pdf (Chunk 22)\n",
      "Content Preview: --- Lecture Page 48 ---\n",
      "\n",
      "Basic COCOMO model\n",
      "• Example – Consider a software project using semi-detached mode \n",
      "with 300 Kloc .find out effort estimatio...\n",
      "\n",
      "Source 4: Lecture#7.pdf (Chunk 22)\n",
      "Content Preview: --- Lecture Page 48 ---\n",
      "\n",
      "Basic COCOMO model\n",
      "• Example – Consider a software project using semi-detached mode \n",
      "with 300 Kloc .find out effort estimatio...\n",
      "\n",
      "Source 5: Lecture#7.pdf (Chunk 22)\n",
      "Content Preview: --- Lecture Page 48 ---\n",
      "\n",
      "Basic COCOMO model\n",
      "• Example – Consider a software project using semi-detached mode \n",
      "with 300 Kloc .find out effort estimatio...\n",
      "\n",
      "Conversation History: 1 exchanges stored\n"
     ]
    }
   ],
   "source": [
    "def ask_study_question(question: str):\n",
    "    \"\"\"\n",
    "    Ask a study-related question using the conversational RAG pipeline\n",
    "    \"\"\"\n",
    "    if 'rag_chain' not in locals() and 'rag_chain' not in globals():\n",
    "        print(\"Study Assistant not available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Get response from conversational RAG chain\n",
    "        result = rag_chain({\"question\": question})\n",
    "        \n",
    "        print(f\"Student Question: {question}\")\n",
    "        print(f\"\\nStudy Assistant Answer:\\n{result['answer']}\")\n",
    "        \n",
    "        # Show source materials referenced\n",
    "        print(f\"\\nSource Materials Referenced ({len(result['source_documents'])})\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, doc in enumerate(result['source_documents'], 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            chunk_id = doc.metadata.get('chunk_id', 'N/A')\n",
    "            print(f\"\\nSource {i}: {source} (Chunk {chunk_id})\")\n",
    "            print(f\"Content Preview: {doc.page_content[:150]}...\")\n",
    "            \n",
    "        # Show current conversation history length\n",
    "        if hasattr(rag_chain, 'memory') and rag_chain.memory:\n",
    "            history = rag_chain.memory.chat_memory.messages\n",
    "            print(f\"\\nConversation History: {len(history)//2} exchanges stored\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during study query: {e}\")\n",
    "\n",
    "def show_conversation_history():\n",
    "    \"\"\"\n",
    "    Display the current conversation history\n",
    "    \"\"\"\n",
    "    if 'rag_chain' not in locals() and 'rag_chain' not in globals():\n",
    "        print(\"Study Assistant not available\")\n",
    "        return\n",
    "        \n",
    "    if hasattr(rag_chain, 'memory') and rag_chain.memory:\n",
    "        history = rag_chain.memory.chat_memory.messages\n",
    "        print(f\"Conversation History ({len(history)//2} exchanges):\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i in range(0, len(history), 2):\n",
    "            if i + 1 < len(history):\n",
    "                question = history[i].content\n",
    "                answer = history[i + 1].content\n",
    "                print(f\"\\nQ{(i//2)+1}: {question}\")\n",
    "                print(f\"A{(i//2)+1}: {answer[:200]}...\")\n",
    "                print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"No conversation history available\")\n",
    "\n",
    "def clear_conversation_history():\n",
    "    \"\"\"\n",
    "    Clear the conversation history to start fresh\n",
    "    \"\"\"\n",
    "    if 'rag_chain' not in locals() and 'rag_chain' not in globals():\n",
    "        print(\"Study Assistant not available\")\n",
    "        return\n",
    "        \n",
    "    if hasattr(rag_chain, 'memory') and rag_chain.memory:\n",
    "        rag_chain.memory.clear()\n",
    "        print(\"Conversation history cleared. Starting fresh!\")\n",
    "    else:\n",
    "        print(\"No conversation memory to clear\")\n",
    "\n",
    "# Test the Study Assistant with a sample question\n",
    "if 'rag_chain' in locals():\n",
    "    # Test with a typical software engineering question\n",
    "    ask_study_question(\"What's semi-detached mode of COCOMO? what formuala are used to calculate effort in this mode?\")\n",
    "else:\n",
    "    print(\"Study Assistant not ready for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae045830",
   "metadata": {},
   "source": [
    "## Interactive Conversational Q&A Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1e01279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Conversational Study Session ===\n",
      "Student Question: What is the COCOMO model?\n",
      "\n",
      "Study Assistant Answer:\n",
      "The COCOMO (Constructive Cost Model) is a procedural software cost estimation model.  As described in the provided lecture notes (Pages 44-45), it's actually a family of models, offering three increasingly detailed levels of accuracy:\n",
      "\n",
      "1. **Basic COCOMO:** This is the simplest model, providing a rapid, high-level estimate of development effort and time.  It uses a single equation to estimate effort based on the size of the project (usually measured in lines of code).  The provided text doesn't detail the specific equation, but that would be found in further lecture notes or the textbook.\n",
      "\n",
      "2. **Intermediate COCOMO:** This model builds upon the basic model by incorporating various attributes of the software project and development environment that influence the development effort. These attributes are called cost drivers and are used to adjust the basic COCOMO estimate.  Again, the specific cost drivers and their impact aren't detailed here.  More information would be needed from your course materials to understand this model fully.\n",
      "\n",
      "3. **Detailed COCOMO:** This is the most comprehensive model, offering the most accurate estimates. It further refines the intermediate model by considering individual modules within the project and their characteristics.  This level of detail allows for a more precise estimation of effort and schedule for each part of the project.  This also requires more detailed information not provided in the given text.\n",
      "\n",
      "The lecture notes (Page 44) also mention an \"Embedded Mode\" within the context of COCOMO I (presumably referring to a version or aspect of the Basic or Intermediate COCOMO).  This mode is characterized by fixed resource requirements, tight constraints, high complexity, and a need for a larger team than other modes.  However, the provided text lacks the details to fully explain how this mode affects the COCOMO estimation process.\n",
      "\n",
      "To fully understand the COCOMO model and its different versions, including the specific equations and cost drivers, you need to consult your complete lecture notes and textbook.  The provided excerpts only give a very high-level overview.\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 2: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 3: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 4: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 5: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Conversation History: 2 exchanges stored\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Student Question: What are its different modes?\n",
      "\n",
      "Study Assistant Answer:\n",
      "Based on the provided lecture notes, the only COCOMO mode explicitly mentioned is the **Embedded Mode**.  The notes describe it as a software project type with:\n",
      "\n",
      "* **Fixed resource requirements:**  The project has predetermined limits on the resources (time, budget, personnel) available.\n",
      "* **Tight constraints:**  Development operates under strict deadlines and limitations.\n",
      "* **High complexity and experience:** The project demands a high level of skill, innovation, and expertise.\n",
      "* **Larger team size:**  Compared to other COCOMO project types (which aren't defined in this excerpt), Embedded Mode projects necessitate a larger development team.\n",
      "\n",
      "\n",
      "The lecture notes also mention three *types* of COCOMO models: Basic, Intermediate, and Detailed.  These are levels of detail and accuracy in the estimation process, not different *modes* of operation like the Embedded Mode.  The notes don't provide information on other potential modes within the COCOMO model. To learn about other potential modes, you would need to consult additional course materials or the full COCOMO documentation.\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 2: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 3: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 4: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 5: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Conversation History: 3 exchanges stored\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Student Question: Can you give me the formula for effort calculation in basic mode?\n",
      "\n",
      "Study Assistant Answer:\n",
      "The provided text only describes the Intermediate COCOMO model's effort calculation formula:  `EffortE = a * KLOC^b * EAF`.  It does *not* provide the formula for the Basic COCOMO model.  To answer your question about the Basic COCOMO model's effort calculation, I need access to the lecture notes or study materials that describe the Basic COCOMO model.\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: Lecture#7.pdf (Chunk 23)\n",
      "Content Preview: --- Lecture Page 49 ---\n",
      "\n",
      "Intermediate COCOMO Model\n",
      "• Intermediate COCOMO model is an extension of the Basic COCOMO \n",
      "model which includes a set of cost...\n",
      "\n",
      "Source 2: Lecture#7.pdf (Chunk 23)\n",
      "Content Preview: --- Lecture Page 49 ---\n",
      "\n",
      "Intermediate COCOMO Model\n",
      "• Intermediate COCOMO model is an extension of the Basic COCOMO \n",
      "model which includes a set of cost...\n",
      "\n",
      "Source 3: Lecture#7.pdf (Chunk 23)\n",
      "Content Preview: --- Lecture Page 49 ---\n",
      "\n",
      "Intermediate COCOMO Model\n",
      "• Intermediate COCOMO model is an extension of the Basic COCOMO \n",
      "model which includes a set of cost...\n",
      "\n",
      "Source 4: Lecture#7.pdf (Chunk 23)\n",
      "Content Preview: --- Lecture Page 49 ---\n",
      "\n",
      "Intermediate COCOMO Model\n",
      "• Intermediate COCOMO model is an extension of the Basic COCOMO \n",
      "model which includes a set of cost...\n",
      "\n",
      "Source 5: Lecture#7.pdf (Chunk 23)\n",
      "Content Preview: --- Lecture Page 49 ---\n",
      "\n",
      "Intermediate COCOMO Model\n",
      "• Intermediate COCOMO model is an extension of the Basic COCOMO \n",
      "model which includes a set of cost...\n",
      "\n",
      "Conversation History: 4 exchanges stored\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Conversation History (4 exchanges):\n",
      "============================================================\n",
      "\n",
      "Q1: What's semi-detached mode of COCOMO? what formuala are used to calculate effort in this mode?\n",
      "A1: The provided text only gives an example calculation using the Basic COCOMO model in what it refers to as \"semi-detached mode,\" but it doesn't define what \"semi-detached mode\" actually means within the...\n",
      "----------------------------------------\n",
      "\n",
      "Q2: What is the COCOMO model?\n",
      "A2: The COCOMO (Constructive Cost Model) is a procedural software cost estimation model.  As described in the provided lecture notes (Pages 44-45), it's actually a family of models, offering three increas...\n",
      "----------------------------------------\n",
      "\n",
      "Q3: What are its different modes?\n",
      "A3: Based on the provided lecture notes, the only COCOMO mode explicitly mentioned is the **Embedded Mode**.  The notes describe it as a software project type with:\n",
      "\n",
      "* **Fixed resource requirements:**  Th...\n",
      "----------------------------------------\n",
      "\n",
      "Q4: Can you give me the formula for effort calculation in basic mode?\n",
      "A4: The provided text only describes the Intermediate COCOMO model's effort calculation formula:  `EffortE = a * KLOC^b * EAF`.  It does *not* provide the formula for the Basic COCOMO model.  To answer yo...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Start a conversational study session\n",
    "# Ask sequential questions to see how the assistant remembers context\n",
    "\n",
    "if 'rag_chain' in locals():\n",
    "    # First question\n",
    "    print(\"=== Starting Conversational Study Session ===\")\n",
    "    ask_study_question(\"What is the COCOMO model?\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Follow-up question that references the previous answer\n",
    "    ask_study_question(\"What are its different modes?\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Another follow-up that builds on previous context\n",
    "    ask_study_question(\"Can you give me the formula for effort calculation in basic mode?\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Show conversation history\n",
    "    show_conversation_history()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nStudy Assistant not ready. Please ensure all previous cells ran successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc698abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asking follow-up: How is the basic mode different from the semi-detached mode we discussed?\n",
      "================================================================================\n",
      "Student Question: How is the basic mode different from the semi-detached mode we discussed?\n",
      "\n",
      "Study Assistant Answer:\n",
      "The provided study materials describe three COCOMO models (Basic, Intermediate, and Detailed) but make no mention of a \"semi-detached COCOMO mode\".  There's no information in the given text to explain the differences between the Basic COCOMO model and a non-existent model.\n",
      "\n",
      "To answer your question accurately, I need additional information.  Specifically, I need access to the lecture notes or textbook that defines \"semi-detached COCOMO mode\".  It's possible this is a typo, a term used in a different context, or from a different software estimation model altogether.  Please provide the relevant materials.\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 2: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 3: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 4: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Source 5: Lecture#7.pdf (Chunk 20)\n",
      "Content Preview: --- Lecture Page 44 ---\n",
      "\n",
      "COCOMO I\n",
      "• Embedded Mode: A software project is said to be an Embedded \n",
      "mode type if-\n",
      "• A software project has fixed requirem...\n",
      "\n",
      "Conversation History: 5 exchanges stored\n",
      "\n",
      "==================================================\n",
      "Conversation Summary:\n",
      "Conversation History (5 exchanges):\n",
      "============================================================\n",
      "\n",
      "Q1: What's semi-detached mode of COCOMO? what formuala are used to calculate effort in this mode?\n",
      "A1: The provided text only gives an example calculation using the Basic COCOMO model in what it refers to as \"semi-detached mode,\" but it doesn't define what \"semi-detached mode\" actually means within the...\n",
      "----------------------------------------\n",
      "\n",
      "Q2: What is the COCOMO model?\n",
      "A2: The COCOMO (Constructive Cost Model) is a procedural software cost estimation model.  As described in the provided lecture notes (Pages 44-45), it's actually a family of models, offering three increas...\n",
      "----------------------------------------\n",
      "\n",
      "Q3: What are its different modes?\n",
      "A3: Based on the provided lecture notes, the only COCOMO mode explicitly mentioned is the **Embedded Mode**.  The notes describe it as a software project type with:\n",
      "\n",
      "* **Fixed resource requirements:**  Th...\n",
      "----------------------------------------\n",
      "\n",
      "Q4: Can you give me the formula for effort calculation in basic mode?\n",
      "A4: The provided text only describes the Intermediate COCOMO model's effort calculation formula:  `EffortE = a * KLOC^b * EAF`.  It does *not* provide the formula for the Basic COCOMO model.  To answer yo...\n",
      "----------------------------------------\n",
      "\n",
      "Q5: How is the basic mode different from the semi-detached mode we discussed?\n",
      "A5: The provided study materials describe three COCOMO models (Basic, Intermediate, and Detailed) but make no mention of a \"semi-detached COCOMO mode\".  There's no information in the given text to explain...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ask your own follow-up questions here\n",
    "your_question = \"How is the basic mode different from the semi-detached mode we discussed?\"  # Modify this\n",
    "\n",
    "if 'rag_chain' in locals():\n",
    "    print(f\"\\nAsking follow-up: {your_question}\")\n",
    "    print(\"=\" * 80)\n",
    "    ask_study_question(your_question)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Conversation Summary:\")\n",
    "    show_conversation_history()\n",
    "else:\n",
    "    print(\"\\nStudy Assistant not ready. Please ensure all previous cells ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70297508",
   "metadata": {},
   "source": [
    "## Conversational Software Engineering Study Assistant Summary\n",
    "\n",
    "This notebook now implements a **conversational** study assistant for software engineering students with full memory capabilities:\n",
    "\n",
    "### New Conversational Features:\n",
    "1. **Conversation Memory** - Remembers last 5 question-answer exchanges using `ConversationBufferWindowMemory`\n",
    "2. **Context Awareness** - LLM can reference previous questions and build upon earlier responses\n",
    "3. **Follow-up Questions** - Ask related questions without repeating context\n",
    "4. **Chat History Management** - View, track, and clear conversation history\n",
    "\n",
    "### Core Technologies:\n",
    "1. **PyMuPDF** - Extracts text from lecture notes, textbooks, and previous year papers\n",
    "2. **RecursiveCharacterTextSplitter** - Creates intelligent chunks for better knowledge retrieval\n",
    "3. **Sentence Transformers** (all-MiniLM-L6-v2) - Semantic understanding of technical concepts\n",
    "4. **ChromaDB** - Fast search across your entire study material collection\n",
    "5. **ConversationalRetrievalChain** - Memory-enabled retrieval with conversation context\n",
    "6. **Gemini Pro** - Provides detailed explanations with conversation history awareness\n",
    "\n",
    "### Conversational Functions:\n",
    "- `ask_study_question(question)` - Ask questions with memory\n",
    "- `show_conversation_history()` - View your chat history  \n",
    "- `clear_conversation_history()` - Start a fresh conversation\n",
    "\n",
    "### Perfect for Natural Study Sessions:\n",
    "- **Follow-up Questions**: \"What about the intermediate mode?\" (after asking about COCOMO)\n",
    "- **Building Context**: \"Can you give an example?\" (after explanations)\n",
    "- **Comparative Questions**: \"How is this different from what we discussed earlier?\"\n",
    "- **Clarifications**: \"Can you explain that last part in more detail?\"\n",
    "\n",
    "### Example Conversational Flow:\n",
    "1. \"What is COCOMO model?\"\n",
    "2. \"What are its different modes?\" \n",
    "3. \"Can you give me the formula for basic mode?\"\n",
    "4. \"How is it different from intermediate mode?\"\n",
    "\n",
    "The assistant remembers all previous exchanges and provides contextually aware responses!\n",
    "\n",
    "**To get started:**\n",
    "1. Get your free Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
    "2. Replace the API key in the setup cell\n",
    "3. Add your lecture notes PDFs to the `pdf_paths` list\n",
    "4. Run all cells to build your conversational knowledge base\n",
    "5. Start asking questions and enjoy natural conversation flow!\n",
    "\n",
    "**Pro Tips:**\n",
    "- Memory stores last 5 exchanges automatically\n",
    "- Ask follow-up questions naturally\n",
    "- Use `show_conversation_history()` to review your session\n",
    "- Clear history with `clear_conversation_history()` for new topics\n",
    "- Perfect for deep-dive study sessions on complex topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac5070",
   "metadata": {},
   "source": [
    "## Conversational Features Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7321609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conversational Study Assistant Features ===\n",
      "\n",
      "1. ask_study_question(question) - Ask questions with memory\n",
      "2. show_conversation_history() - View chat history\n",
      "3. clear_conversation_history() - Start fresh conversation\n",
      "\n",
      "=== Example Conversational Flow ===\n",
      "Try asking related questions to see how the assistant remembers context:\n",
      "1. 'What is COCOMO model?'\n",
      "2. 'What are its different modes?'\n",
      "3. 'Can you give me the formula for the basic mode?'\n",
      "4. 'How is it different from the intermediate mode?'\n",
      "\n",
      "Current conversation history: 5 exchanges\n",
      "\n",
      "The assistant will remember previous questions and build upon them!\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of conversational features\n",
    "if 'rag_chain' in locals():\n",
    "    print(\"=== Conversational Study Assistant Features ===\")\n",
    "    print(\"\\n1. ask_study_question(question) - Ask questions with memory\")\n",
    "    print(\"2. show_conversation_history() - View chat history\")\n",
    "    print(\"3. clear_conversation_history() - Start fresh conversation\")\n",
    "    \n",
    "    print(\"\\n=== Example Conversational Flow ===\")\n",
    "    print(\"Try asking related questions to see how the assistant remembers context:\")\n",
    "    print(\"1. 'What is COCOMO model?'\")\n",
    "    print(\"2. 'What are its different modes?'\") \n",
    "    print(\"3. 'Can you give me the formula for the basic mode?'\")\n",
    "    print(\"4. 'How is it different from the intermediate mode?'\")\n",
    "    \n",
    "    # Show current memory state\n",
    "    if hasattr(rag_chain, 'memory') and rag_chain.memory:\n",
    "        history_count = len(rag_chain.memory.chat_memory.messages) // 2\n",
    "        print(f\"\\nCurrent conversation history: {history_count} exchanges\")\n",
    "    \n",
    "    print(\"\\nThe assistant will remember previous questions and build upon them!\")\n",
    "else:\n",
    "    print(\"Study Assistant not ready for conversational features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9fe2067-8d95-44ba-a9c7-591a76f4e382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Question: What's greedy algorithm?\n",
      "\n",
      "Study Assistant Answer:\n",
      "A greedy algorithm is a simple, intuitive algorithm that makes the locally optimal choice at each stage with the hope of finding a global optimum.  It doesn't consider the big picture or future consequences; it simply makes the best choice available *right now*.  This approach works well for problems exhibiting two key properties:\n",
      "\n",
      "1. **Greedy Choice Property:**  This means that there's always an optimal solution that includes the best immediate choice.  In other words, making the locally optimal choice at each step doesn't prevent us from finding the globally optimal solution.  We can make the choice that seems best at the moment and then solve the remaining subproblem.  This leads to a top-down approach where the problem is iteratively reduced to smaller subproblems.\n",
      "\n",
      "2. **Optimal Substructure Property:** This means that an optimal solution to the problem contains within it optimal solutions to its subproblems.  If we find the optimal solution for a subproblem, we can use that solution as part of the overall optimal solution.\n",
      "\n",
      "**Example:**\n",
      "\n",
      "Consider the problem of finding the minimum spanning tree (MST) of a graph using Prim's algorithm.  Prim's algorithm is a greedy algorithm.\n",
      "\n",
      "* **Greedy Choice:** At each step, it selects the edge with the minimum weight that connects a vertex already in the MST to a vertex not yet in the MST.  This is the locally optimal choice.\n",
      "\n",
      "* **Greedy Choice Property:**  Prim's algorithm satisfies this property because it can be proven that there exists an MST that includes the edge with the minimum weight connecting the current MST to the rest of the graph.\n",
      "\n",
      "* **Optimal Substructure Property:**  An MST of a graph contains within it MSTs of its subgraphs.  If we have an MST for a subgraph, we can extend it to create an MST for the larger graph by carefully adding edges.\n",
      "\n",
      "**Important Note:**  Not all optimization problems can be solved using greedy algorithms.  While many problems exhibit these properties, there's no guarantee that a greedy approach will always find the global optimum.  For some problems, a greedy approach might lead to a suboptimal solution.  The key is to determine if the problem at hand possesses the Greedy Choice and Optimal Substructure properties before attempting a greedy solution.  If these properties aren't present, a different algorithmic approach (like dynamic programming) might be necessary.\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Source 2: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Source 3: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Source 4: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Source 5: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Conversation History: 6 exchanges stored\n"
     ]
    }
   ],
   "source": [
    "ask_study_question(\"What's greedy algorithm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0077d08-9139-45da-b129-689768967f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Question: could you please explain more simply in simpler words, perhaps with an example\n",
      "\n",
      "Study Assistant Answer:\n",
      "Okay, let's simplify the explanation of greedy algorithms.\n",
      "\n",
      "Imagine you're making a change with coins (say, US currency). You want to use the fewest number of coins possible.  A greedy approach would be to always choose the largest coin denomination that's less than or equal to the remaining amount.\n",
      "\n",
      "For example, if you need to make change for $0.87:\n",
      "\n",
      "1. You'd first take a $0.50 coin (the largest coin less than or equal to $0.87).\n",
      "2. You'd have $0.37 left.  The next largest coin is a $0.25 coin.\n",
      "3. You'd have $0.12 left.  The next largest is a $0.10 coin.\n",
      "4. You'd have $0.02 left.  Two $0.01 coins complete the change.\n",
      "\n",
      "This is a greedy algorithm because at each step, you make the choice that seems best *at that moment* – choosing the largest possible coin.  You don't look ahead to see if a different choice might lead to fewer coins overall.  In this case, the greedy approach happens to give you the optimal solution (the fewest number of coins).\n",
      "\n",
      "However, greedy algorithms don't *always* find the best solution.  The success depends heavily on the problem.  The key characteristics of a greedy algorithm are:\n",
      "\n",
      "* **Local Optimization:** It makes the best choice at each step, without considering the long-term consequences.\n",
      "* **No Backtracking:** It doesn't reconsider previous choices.\n",
      "* **Stepwise Problem Reduction:**  Each step simplifies the problem, moving closer to a solution.\n",
      "\n",
      "The coin change example is a good illustration because it's easy to understand.  However,  keep in mind that for more complex problems, a greedy approach might not yield the optimal solution.  The provided lecture notes emphasize this point:  \"For many optimization problems, a greedy algorithm can be used. (not always).\"\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: GreedyAlgorithms.pdf (Chunk 0)\n",
      "Content Preview: --- Lecture Page 1 ---\n",
      "\n",
      "CSE 301\n",
      "Combinatorial Optimization\n",
      "Greedy Algorithms\n",
      "\n",
      "\n",
      "--- Lecture Page 2 ---\n",
      "\n",
      "Greedy Algorithm\n",
      "• Solves an optimization probl...\n",
      "\n",
      "Source 2: GreedyAlgorithms.pdf (Chunk 0)\n",
      "Content Preview: --- Lecture Page 1 ---\n",
      "\n",
      "CSE 301\n",
      "Combinatorial Optimization\n",
      "Greedy Algorithms\n",
      "\n",
      "\n",
      "--- Lecture Page 2 ---\n",
      "\n",
      "Greedy Algorithm\n",
      "• Solves an optimization probl...\n",
      "\n",
      "Source 3: GreedyAlgorithms.pdf (Chunk 0)\n",
      "Content Preview: --- Lecture Page 1 ---\n",
      "\n",
      "CSE 301\n",
      "Combinatorial Optimization\n",
      "Greedy Algorithms\n",
      "\n",
      "\n",
      "--- Lecture Page 2 ---\n",
      "\n",
      "Greedy Algorithm\n",
      "• Solves an optimization probl...\n",
      "\n",
      "Source 4: GreedyAlgorithms.pdf (Chunk 0)\n",
      "Content Preview: --- Lecture Page 1 ---\n",
      "\n",
      "CSE 301\n",
      "Combinatorial Optimization\n",
      "Greedy Algorithms\n",
      "\n",
      "\n",
      "--- Lecture Page 2 ---\n",
      "\n",
      "Greedy Algorithm\n",
      "• Solves an optimization probl...\n",
      "\n",
      "Source 5: GreedyAlgorithms.pdf (Chunk 0)\n",
      "Content Preview: --- Lecture Page 1 ---\n",
      "\n",
      "CSE 301\n",
      "Combinatorial Optimization\n",
      "Greedy Algorithms\n",
      "\n",
      "\n",
      "--- Lecture Page 2 ---\n",
      "\n",
      "Greedy Algorithm\n",
      "• Solves an optimization probl...\n",
      "\n",
      "Conversation History: 7 exchanges stored\n"
     ]
    }
   ],
   "source": [
    "ask_study_question(\"could you please explain more simply in simpler words, perhaps with an example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efe24dab-e285-46c9-b23c-4b972030f50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Question: what if a question cannot be solved with this algorithm?\n",
      "\n",
      "Study Assistant Answer:\n",
      "The provided study material focuses solely on identifying whether a greedy algorithm *can* be used to solve a problem.  It doesn't offer alternatives when a greedy approach is unsuitable.  To answer your question about alternative algorithms, we need additional information on the type of optimization problem you're facing.\n",
      "\n",
      "However, I can give you some general categories of algorithms that are often used when a greedy approach fails:\n",
      "\n",
      "* **Dynamic Programming:**  If the problem exhibits optimal substructure (like greedy problems), but the greedy choice property doesn't hold, dynamic programming is a powerful technique.  It systematically explores all possible subproblem solutions and combines them to find the optimal solution for the overall problem.  This is often more computationally expensive than a greedy algorithm but guarantees an optimal solution.  Examples include the knapsack problem (certain variations) and sequence alignment.\n",
      "\n",
      "* **Divide and Conquer:** This approach recursively breaks down a problem into smaller subproblems, solves them independently, and then combines the solutions.  Merge sort and quicksort are classic examples.  It's useful when the problem can be naturally divided into smaller, similar subproblems.\n",
      "\n",
      "* **Branch and Bound:** This technique explores a tree of possible solutions, pruning branches that are guaranteed to not lead to a better solution than the current best. It's often used for NP-hard problems where finding the absolute best solution is computationally infeasible, but a good approximation is acceptable.\n",
      "\n",
      "* **Backtracking:** This explores possible solutions incrementally, and backtracks when a partial solution is found to be invalid or unpromising.  It's useful for problems where the solution space can be represented as a tree.  The N-Queens problem is a classic example.\n",
      "\n",
      "* **Approximation Algorithms:** For NP-hard problems where finding the optimal solution is computationally intractable, approximation algorithms aim to find a solution that is close to optimal within a guaranteed bound.\n",
      "\n",
      "To recommend a specific alternative, please provide more details about the problem you're trying to solve.  What is the problem statement? What are the constraints?  What have you tried so far?  With more information, I can offer more tailored advice.\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Source 2: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Source 3: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Source 4: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Source 5: GreedyAlgorithms.pdf (Chunk 11)\n",
      "Content Preview: Properties of Greedy Problems\n",
      "How can one tell if a greedy algorithm will be able to solve an \n",
      "optimization problem i.e., whether a problem is a “Gree...\n",
      "\n",
      "Conversation History: 8 exchanges stored\n"
     ]
    }
   ],
   "source": [
    "ask_study_question(\"what if a question cannot be solved with this algorithm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86104a1-a7af-4a36-953b-15ed1b62fc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
