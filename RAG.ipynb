{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3750a8-791b-4bd8-a887-2684fc4b3a43",
   "metadata": {},
   "source": [
    "# Software Engineering Study Assistant - RAG Pipeline\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) chatbot designed to help software engineering students with:\n",
    "- **Understanding complex topics** from lecture notes and textbooks\n",
    "- **Solving previous year exam questions** with detailed explanations\n",
    "- **Getting contextual answers** from course materials and PDFs\n",
    "- **Study assistance** with proper references to source materials\n",
    "\n",
    "**Technology Stack:**\n",
    "- **PyMuPDF** for PDF lecture notes extraction\n",
    "- **LangChain's RecursiveCharacterTextSplitter** for intelligent text chunking\n",
    "- **Sentence Transformers** (all-MiniLM-L6-v2) for semantic embeddings\n",
    "- **ChromaDB** for fast similarity search across study materials\n",
    "- **LangChain's retriever** for relevant content retrieval\n",
    "- **Gemini Pro** for generating comprehensive answers with context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac40ff48-d0a0-4c31-8b82-3bc1551f7801",
   "metadata": {},
   "source": [
    "## Study Assistant Pipeline Flow\n",
    "\n",
    "```\n",
    "Lecture Notes PDFs → PyMuPDF → Text Extraction → RecursiveCharacterTextSplitter → Knowledge Chunks\n",
    "                                                           ↓\n",
    "                                              Sentence Transformers → Semantic Embeddings → ChromaDB Knowledge Base\n",
    "                                                           ↓\n",
    "Student Question/Problem → Query Embedding → LangChain Retriever → Relevant Study Materials\n",
    "                                                           ↓\n",
    "                        Gemini Pro ← Context + Question → Detailed Answer with References\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- \"Explain object-oriented programming concepts\"\n",
    "- \"How do I solve this data structures problem?\"\n",
    "- \"What are the key points about software testing methodologies?\"\n",
    "- \"Help me understand this previous year question on algorithms\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa5e3c-5486-4fc6-b316-958d3bc895ac",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install all required packages using the requirements.txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2257b5fe-79df-4791-9c18-3ab2222d7330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF>=1.23.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.26.3)\n",
      "Requirement already satisfied: langchain-community in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.3.27)\n",
      "Requirement already satisfied: langchain>=0.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 9)) (0.3.27)\n",
      "Requirement already satisfied: langchain-google-genai>=1.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2.0.10)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 13)) (5.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 14)) (2.8.0)\n",
      "Requirement already satisfied: transformers>=4.30.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 15)) (4.55.0)\n",
      "Requirement already satisfied: chromadb>=0.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 18)) (1.0.16)\n",
      "Requirement already satisfied: google-generativeai>=0.3.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 21)) (0.8.5)\n",
      "Requirement already satisfied: numpy>=1.24.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 24)) (2.3.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 25)) (2.3.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 26)) (4.67.1)\n",
      "Requirement already satisfied: jupyter>=1.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 29)) (1.1.1)\n",
      "Requirement already satisfied: ipykernel>=6.25.0 in d:\\study-assistant\\venv\\lib\\site-packages (from -r requirements.txt (line 30)) (6.30.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (0.3.74)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (0.4.13)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain>=0.1.0->-r requirements.txt (line 9)) (0.3.9)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain>=0.1.0->-r requirements.txt (line 9)) (2.11.7)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-google-genai>=1.0.0->-r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\study-assistant\\venv\\lib\\site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (1.7.1)\n",
      "Requirement already satisfied: scipy in d:\\study-assistant\\venv\\lib\\site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\study-assistant\\venv\\lib\\site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (0.34.4)\n",
      "Requirement already satisfied: Pillow in d:\\study-assistant\\venv\\lib\\site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\study-assistant\\venv\\lib\\site-packages (from sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (4.14.1)\n",
      "Requirement already satisfied: filelock in d:\\study-assistant\\venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 14)) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\study-assistant\\venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 14)) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\study-assistant\\venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 14)) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\study-assistant\\venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 14)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\study-assistant\\venv\\lib\\site-packages (from torch>=2.0.0->-r requirements.txt (line 14)) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\study-assistant\\venv\\lib\\site-packages (from transformers>=4.30.0->-r requirements.txt (line 15)) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\study-assistant\\venv\\lib\\site-packages (from transformers>=4.30.0->-r requirements.txt (line 15)) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\study-assistant\\venv\\lib\\site-packages (from transformers>=4.30.0->-r requirements.txt (line 15)) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\study-assistant\\venv\\lib\\site-packages (from transformers>=4.30.0->-r requirements.txt (line 15)) (0.6.2)\n",
      "Requirement already satisfied: build>=1.0.3 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in d:\\study-assistant\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 18)) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (1.36.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (3.11.2)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in d:\\study-assistant\\venv\\lib\\site-packages (from chromadb>=0.4.0->-r requirements.txt (line 18)) (4.25.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in d:\\study-assistant\\venv\\lib\\site-packages (from google-generativeai>=0.3.0->-r requirements.txt (line 21)) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in d:\\study-assistant\\venv\\lib\\site-packages (from google-generativeai>=0.3.0->-r requirements.txt (line 21)) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in d:\\study-assistant\\venv\\lib\\site-packages (from google-generativeai>=0.3.0->-r requirements.txt (line 21)) (2.178.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in d:\\study-assistant\\venv\\lib\\site-packages (from google-generativeai>=0.3.0->-r requirements.txt (line 21)) (2.40.3)\n",
      "Requirement already satisfied: protobuf in d:\\study-assistant\\venv\\lib\\site-packages (from google-generativeai>=0.3.0->-r requirements.txt (line 21)) (5.29.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\study-assistant\\venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\study-assistant\\venv\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 25)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\study-assistant\\venv\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 25)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\study-assistant\\venv\\lib\\site-packages (from pandas>=2.0.0->-r requirements.txt (line 25)) (2025.2)\n",
      "Requirement already satisfied: colorama in d:\\study-assistant\\venv\\lib\\site-packages (from tqdm>=4.65.0->-r requirements.txt (line 26)) (0.4.6)\n",
      "Requirement already satisfied: notebook in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter>=1.0.0->-r requirements.txt (line 29)) (7.4.5)\n",
      "Requirement already satisfied: jupyter-console in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter>=1.0.0->-r requirements.txt (line 29)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter>=1.0.0->-r requirements.txt (line 29)) (7.16.6)\n",
      "Requirement already satisfied: ipywidgets in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter>=1.0.0->-r requirements.txt (line 29)) (8.1.7)\n",
      "Requirement already satisfied: jupyterlab in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter>=1.0.0->-r requirements.txt (line 29)) (4.4.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (9.4.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (27.0.1)\n",
      "Requirement already satisfied: tornado>=6.2 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from ipykernel>=6.25.0->-r requirements.txt (line 30)) (5.14.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\study-assistant\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 8)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 8)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\study-assistant\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 8)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\study-assistant\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\study-assistant\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 8)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 8)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\study-assistant\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 8)) (1.20.1)\n",
      "Requirement already satisfied: pyproject_hooks in d:\\study-assistant\\venv\\lib\\site-packages (from build>=1.0.3->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\study-assistant\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 8)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 8)) (0.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in d:\\study-assistant\\venv\\lib\\site-packages (from google-api-core->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\study-assistant\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\study-assistant\\venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (4.9.1)\n",
      "Requirement already satisfied: anyio in d:\\study-assistant\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (4.10.0)\n",
      "Requirement already satisfied: certifi in d:\\study-assistant\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\study-assistant\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\study-assistant\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\study-assistant\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (0.16.0)\n",
      "Requirement already satisfied: decorator in d:\\study-assistant\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in d:\\study-assistant\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\study-assistant\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\study-assistant\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in d:\\study-assistant\\venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (0.6.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\study-assistant\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\study-assistant\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\study-assistant\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (0.27.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.25.0->-r requirements.txt (line 30)) (4.3.8)\n",
      "Requirement already satisfied: pywin32>=300 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.25.0->-r requirements.txt (line 30)) (311)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\study-assistant\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.17.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\study-assistant\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in d:\\study-assistant\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in d:\\study-assistant\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in d:\\study-assistant\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in d:\\study-assistant\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\study-assistant\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community->-r requirements.txt (line 8)) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\study-assistant\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain-community->-r requirements.txt (line 8)) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in d:\\study-assistant\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 18)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\study-assistant\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 18)) (25.2.10)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in d:\\study-assistant\\venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in d:\\study-assistant\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in d:\\study-assistant\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in d:\\study-assistant\\venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (0.57b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\study-assistant\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in d:\\study-assistant\\venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\study-assistant\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\study-assistant\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 9)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\study-assistant\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->-r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\study-assistant\\venv\\lib\\site-packages (from requests<3,>=2->langchain-community->-r requirements.txt (line 8)) (3.4.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (4.0.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\study-assistant\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community->-r requirements.txt (line 8)) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 14)) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\study-assistant\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in d:\\study-assistant\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 18)) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\study-assistant\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\study-assistant\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.4.0->-r requirements.txt (line 18)) (15.0.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in d:\\study-assistant\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\study-assistant\\venv\\lib\\site-packages (from google-api-python-client->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (4.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in d:\\study-assistant\\venv\\lib\\site-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 29)) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in d:\\study-assistant\\venv\\lib\\site-packages (from ipywidgets->jupyter>=1.0.0->-r requirements.txt (line 29)) (3.0.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 14)) (3.0.2)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.0.5)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.2.6)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.16.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (65.5.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\study-assistant\\venv\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in d:\\study-assistant\\venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in d:\\study-assistant\\venv\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\study-assistant\\venv\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in d:\\study-assistant\\venv\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in d:\\study-assistant\\venv\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in d:\\study-assistant\\venv\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\study-assistant\\venv\\lib\\site-packages (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.2.0->-r requirements.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: webencodings in d:\\study-assistant\\venv\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.4.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in d:\\study-assistant\\venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (1.71.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in d:\\study-assistant\\venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (3.2.3)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\study-assistant\\venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (3.23.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\study-assistant\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (0.8.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\study-assistant\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.5.3)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.22.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.0.15)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.18.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\study-assistant\\venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (1.3.1)\n",
      "Requirement already satisfied: babel>=2.10 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.12.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\study-assistant\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.4.0->-r requirements.txt (line 18)) (0.1.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in d:\\study-assistant\\venv\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.21.1)\n",
      "Requirement already satisfied: wcwidth in d:\\study-assistant\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (0.2.13)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in d:\\study-assistant\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.3.0->-r requirements.txt (line 21)) (0.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\study-assistant\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\study-assistant\\venv\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\study-assistant\\venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 18)) (10.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\study-assistant\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in d:\\study-assistant\\venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.25.0->-r requirements.txt (line 30)) (0.2.3)\n",
      "Requirement already satisfied: argon2-cffi-bindings in d:\\study-assistant\\venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (25.1.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\study-assistant\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb>=0.4.0->-r requirements.txt (line 18)) (3.5.4)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (3.3.0)\n",
      "Requirement already satisfied: rfc3339-validator in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in d:\\study-assistant\\venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in d:\\study-assistant\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in d:\\study-assistant\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (20.11.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.1.0)\n",
      "Requirement already satisfied: uri-template in d:\\study-assistant\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in d:\\study-assistant\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in d:\\study-assistant\\venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in d:\\study-assistant\\venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.22)\n",
      "Requirement already satisfied: lark>=1.2.2 in d:\\study-assistant\\venv\\lib\\site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.2.2)\n",
      "Requirement already satisfied: arrow>=0.15.0 in d:\\study-assistant\\venv\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in d:\\study-assistant\\venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 29)) (2.9.0.20250809)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for RAG pipeline using requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c15eba-f97e-4555-b3d8-5607ad5dbcb5",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a703d29-9cb8-4040-8296-d5d7fd86090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import uuid\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0dde4d-af1a-4f7e-bb42-6435e1a1ac60",
   "metadata": {},
   "source": [
    "## Gemini API Key Setup\n",
    "\n",
    "Get your free Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f6deb0-ff3c-4efa-a2c5-b2585902df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API key configured\n",
      "API key starts with: AIzaSyCA...\n"
     ]
    }
   ],
   "source": [
    "# Set your Gemini API key\n",
    "GEMINI_API_KEY = \"AIzaSyCATHVWy8gTDiLCZCMcJxmqDw-u33X9cFQ\"  # Replace with your actual API key\n",
    "\n",
    "# Verify API key is set\n",
    "if GEMINI_API_KEY == \"your-gemini-api-key-here\":\n",
    "    print(\"Please replace 'your-gemini-api-key-here' with your actual Gemini API key\")\n",
    "    print(\"Get your free API key from: https://makersuite.google.com/app/apikey\")\n",
    "else:\n",
    "    print(\"Gemini API key configured\")\n",
    "    print(f\"API key starts with: {GEMINI_API_KEY[:8]}...\")\n",
    "\n",
    "# Set environment variable for Google Generative AI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1bf069-b68b-4778-9986-c735eadc6ba0",
   "metadata": {},
   "source": [
    "## PDF Text Extraction with PyMuPDF\n",
    "\n",
    "This section handles text-based PDFs using direct text extraction:\n",
    "\n",
    "- **Text-based PDFs**: Direct text extraction using PyMuPDF for PDFs created from digital documents (Word, LaTeX, Google Docs, etc.)\n",
    "\n",
    "**Supported PDF Types:**\n",
    "- Documents created from Word processors\n",
    "- LaTeX-generated PDFs  \n",
    "- Google Docs exports\n",
    "- Any PDF with embedded text data\n",
    "\n",
    "The pipeline uses PyMuPDF for fast and accurate text extraction from digital documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969fd99f-fd8c-48af-a86f-6d0681f972be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Text-based PDFs ===\n",
      "Processing: metrics3.pdf\n",
      "  → Using direct text extraction\n",
      "Extracted 17980 characters from metrics3.pdf\n",
      "Processing: Lecture#7.pdf\n",
      "  → Using direct text extraction\n",
      "Extracted 19743 characters from Lecture#7.pdf\n",
      "Processing: Sample.pdf\n",
      "  → Using direct text extraction\n",
      "Extracted 20737 characters from Sample.pdf\n",
      "Processing: GreedyAlgorithms.pdf\n",
      "  → Using direct text extraction\n",
      "Extracted 19614 characters from GreedyAlgorithms.pdf\n",
      "\n",
      "=== EXTRACTION SUMMARY ===\n",
      "Total extracted content: 78209 characters\n",
      "PDFs processed: 4\n",
      "First 500 characters:\n",
      "\n",
      "\n",
      "=== SOURCE: metrics3.pdf ===\n",
      "\n",
      "\n",
      "\n",
      "--- Lecture Page 1 ---\n",
      "\n",
      "                                                           \n",
      " \n",
      " \n",
      "Institute of Information Technology \n",
      "University of Dhaka \n",
      " \n",
      " \n",
      "Topic: Function Point Analysis of SPL-2 Project \n",
      "Software Metrics (SE-611) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "Submitted to \n",
      "Dr. Emon Kumar Dey  \n",
      "Associate Professor  \n",
      "IIT, University of Dhaka  \n",
      " \n",
      " \n",
      " \n",
      "Submitted by \n",
      "Md. Shakibul Islam Shakib - BSSE 1404 \n",
      "Nandan Bhowmick - BSSE 1436  \n",
      " \n",
      "\n",
      "\n",
      "--- Lecture Page 2 ---\n",
      "\n",
      "1. Project Overview..........\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from text-based PDFs using PyMuPDF\n",
    "    Use this for PDFs created from digital documents (Word, LaTeX, Google Docs, etc.)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Warning: PDF file not found: {pdf_path}\")\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"Processing: {os.path.basename(pdf_path)}\")\n",
    "    print(f\"  → Using direct text extraction\")\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        page_text = page.get_text()\n",
    "        \n",
    "        if page_text.strip():  # Only add non-empty pages\n",
    "            text += f\"\\n\\n--- Lecture Page {page_num + 1} ---\\n\\n\"\n",
    "            text += page_text\n",
    "    \n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "# Add your text-based PDFs here (created from Word, LaTeX, Google Docs, etc.)\n",
    "pdf_paths = [\n",
    "    \"./assets/metrics3.pdf\", \n",
    "    \"./assets/Lecture#7.pdf\",\n",
    "    \"./assets/Sample.pdf\",\n",
    "    \"./assets/GreedyAlgorithms.pdf\"\n",
    "]\n",
    "\n",
    "all_extracted_text = \"\"\n",
    "\n",
    "print(\"=== Processing Text-based PDFs ===\")\n",
    "for pdf_path in pdf_paths:\n",
    "    if os.path.exists(pdf_path):\n",
    "        extracted_text = extract_text_from_pdf(pdf_path)\n",
    "        all_extracted_text += f\"\\n\\n=== SOURCE: {os.path.basename(pdf_path)} ===\\n\\n\" + extracted_text\n",
    "        print(f\"Extracted {len(extracted_text)} characters from {os.path.basename(pdf_path)}\")\n",
    "    else:\n",
    "        print(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "if all_extracted_text:\n",
    "    print(f\"\\n=== EXTRACTION SUMMARY ===\")\n",
    "    print(f\"Total extracted content: {len(all_extracted_text)} characters\")\n",
    "    print(f\"PDFs processed: {len([p for p in pdf_paths if os.path.exists(p)])}\")\n",
    "    print(f\"First 500 characters:\\n{all_extracted_text[:500]}...\")\n",
    "else:\n",
    "    print(\"\\nNo PDF files were processed.\")\n",
    "    print(\"Please add your text-based PDFs to the pdf_paths list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64396d45-24a6-4f74-bd6f-837192ce30aa",
   "metadata": {},
   "source": [
    "### Understanding Text-based PDF Processing\n",
    "\n",
    "**Text-based PDFs**: Created from digital documents (Word, LaTeX, Google Docs, etc.) - contain actual text data that can be directly extracted.\n",
    "\n",
    "**Processing Features:**\n",
    "- Fast direct text extraction using PyMuPDF\n",
    "- Maintains original text formatting and structure\n",
    "- Works with all standard PDF formats containing embedded text\n",
    "- Preserves page structure with clear page separators\n",
    "\n",
    "**Best Results With:**\n",
    "- Documents created from word processors (Word, Google Docs, etc.)\n",
    "- LaTeX-generated academic papers and textbooks\n",
    "- Exported PDFs from presentation software\n",
    "- Any PDF with selectable/copyable text\n",
    "\n",
    "**Note**: This pipeline is optimized for text-based PDFs. If you have scanned documents (images of text), you would need OCR functionality, which can be added later if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaee036-7fe3-428b-9eae-43b3295a9222",
   "metadata": {},
   "source": [
    "## Text Chunking with LangChain's RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c35b93c7-bdef-4e5f-8712-e8f08fd5d6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 123 knowledge chunks from study materials\n",
      "Average chunk size: 686 characters\n",
      "Sources processed: Unknown, Sample.pdf, Lecture#7.pdf, metrics3.pdf, GreedyAlgorithms.pdf\n",
      "\n",
      "First chunk preview:\n",
      "=== SOURCE: metrics3.pdf ===\n",
      "\n",
      "\n",
      "\n",
      "--- Lecture Page 1 ---\n",
      "\n",
      "                                                           \n",
      " \n",
      " \n",
      "Institute of Information Technology \n",
      "University of Dhaka \n",
      " \n",
      " \n",
      "Topic: Function Point Analysis of SPL-2 Project \n",
      "Software Metrics (SE-611) \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "Submitted to \n",
      "Dr. Emon Kumar ...\n"
     ]
    }
   ],
   "source": [
    "def create_overlapping_chunks(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split lecture notes into overlapping chunks for better retrieval\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    # Split text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Convert to LangChain Documents with educational metadata\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Extract source from chunk if available\n",
    "        source = \"Unknown\"\n",
    "        if \"=== SOURCE:\" in chunk:\n",
    "            lines = chunk.split('\\n')\n",
    "            for line in lines:\n",
    "                if \"=== SOURCE:\" in line:\n",
    "                    source = line.replace(\"=== SOURCE:\", \"\").replace(\"===\", \"\").strip()\n",
    "                    break\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"chunk_id\": i,\n",
    "                \"source\": source,\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"content_type\": \"lecture_notes\"\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Create chunks from all extracted text\n",
    "if 'all_extracted_text' in locals() and all_extracted_text:\n",
    "    documents = create_overlapping_chunks(all_extracted_text)\n",
    "    print(f\"Created {len(documents)} knowledge chunks from study materials\")\n",
    "    print(f\"Average chunk size: {sum(len(doc.page_content) for doc in documents) // len(documents)} characters\")\n",
    "    \n",
    "    # Show sources processed\n",
    "    sources = set(doc.metadata.get('source', 'Unknown') for doc in documents)\n",
    "    print(f\"Sources processed: {', '.join(sources)}\")\n",
    "    \n",
    "    print(f\"\\nFirst chunk preview:\\n{documents[0].page_content[:300]}...\")\n",
    "else:\n",
    "    print(\"No extracted text available for chunking. Please ensure PDFs are properly loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570165e3-f318-4750-9b7c-dff533916910",
   "metadata": {},
   "source": [
    "## Initialize Sentence Transformers for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e03069e3-86c4-41b0-b386-a42c1f63ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8172\\263079578.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Transformers (all-MiniLM-L6-v2) model loaded\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Initialize Sentence Transformers embeddings\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Sentence Transformers (all-MiniLM-L6-v2) model loaded\")\n",
    "print(f\"Embedding dimension: 384\")  # all-MiniLM-L6-v2 produces 384-dimensional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3738d9-c27b-468a-873d-d1bd54c7d643",
   "metadata": {},
   "source": [
    "## Create ChromaDB Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "574200c9-7d97-480e-9fb0-692f629b337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineering Knowledge Base created with 123 chunks\n",
      "Database persisted to: ./chroma_db\n",
      "Your study materials are now ready for questions!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8172\\1586654156.py:14: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "# Set up ChromaDB knowledge base for study materials\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Create or load ChromaDB vector store for educational content\n",
    "if 'documents' in locals() and documents:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_name=\"software_engineering_knowledge_base\"\n",
    "    )\n",
    "    \n",
    "    # Persist the database\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    print(f\"Software Engineering Knowledge Base created with {len(documents)} chunks\")\n",
    "    print(f\"Database persisted to: {persist_directory}\")\n",
    "    print(\"Your study materials are now ready for questions!\")\n",
    "else:\n",
    "    print(\"No study materials available for knowledge base creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc589e7-190f-4625-bf6c-a048c8ae49e0",
   "metadata": {},
   "source": [
    "## Create LangChain Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "844c0f3d-79dc-4a00-890e-98029763e424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study Materials Retriever created\n",
      "Search type: similarity\n",
      "Number of chunks retrieved per query: 5\n",
      "\n",
      "Test retrieval for 'What are the main concepts in object-oriented programming?':\n",
      "Retrieved 5 relevant study materials\n",
      "First retrieved content preview:\n",
      "--- Lecture Page 14 ---\n",
      "\n",
      "Design Size\n",
      "• Object-oriented designs add new abstraction mechanisms: objects, classes, \n",
      "interfaces, operations, methods, associations, inheritance, etc.\n",
      "• Thus, we will measu...\n",
      "Source: Unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8172\\914408655.py:14: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(test_query)\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever for study materials\n",
    "if 'vectorstore' in locals():\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}  # Retrieve top 5 most relevant study materials\n",
    "    )\n",
    "    \n",
    "    print(\"Study Materials Retriever created\")\n",
    "    print(\"Search type: similarity\")\n",
    "    print(\"Number of chunks retrieved per query: 5\")\n",
    "    \n",
    "    # Test the retriever with a typical student question\n",
    "    test_query = \"What are the main concepts in object-oriented programming?\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(test_query)\n",
    "    print(f\"\\nTest retrieval for '{test_query}':\")\n",
    "    print(f\"Retrieved {len(retrieved_docs)} relevant study materials\")\n",
    "    if retrieved_docs:\n",
    "        print(f\"First retrieved content preview:\\n{retrieved_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Source: {retrieved_docs[0].metadata.get('source', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"Knowledge base not available for retriever creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd0a0f",
   "metadata": {},
   "source": [
    "## Initialize Gemini Pro with API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b93a2499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Pro LLM initialized with API key\n",
      "Model: gemini-pro\n",
      "Temperature: 0.3\n",
      "Max output tokens: 1024\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini Pro LLM with API key\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"models/gemini-2.5-pro\",\n",
    "        google_api_key=GEMINI_API_KEY,\n",
    "        temperature=0.3,\n",
    "        max_output_tokens=1024\n",
    "    )\n",
    "    \n",
    "    print(\"Gemini Pro LLM initialized with API key\")\n",
    "    print(f\"Model: gemini-pro\")\n",
    "    print(f\"Temperature: 0.3\")\n",
    "    print(f\"Max output tokens: 1024\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini Pro: {e}\")\n",
    "    print(\"Please ensure you have:\")\n",
    "    print(\"1. Valid Gemini API key\")\n",
    "    print(\"2. Correct API key format\")\n",
    "    print(\"3. Get your key from: https://makersuite.google.com/app/apikey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a6cfa",
   "metadata": {},
   "source": [
    "## Create RAG Chain with Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af54bcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineering Study Assistant created successfully\n",
      "Chain type: stuff (combines all retrieved study materials)\n",
      "Returns source documents: Yes\n",
      "Ready to help with your studies!\n"
     ]
    }
   ],
   "source": [
    "# Define a custom prompt template for educational assistance\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an AI Study Assistant for Software Engineering students. Your role is to help students understand concepts, solve problems, and prepare for exams using their course materials.\n",
    "\n",
    "Instructions:\n",
    "- Provide clear, detailed explanations suitable for students\n",
    "- Include examples when helpful for understanding\n",
    "- Reference the source materials when possible\n",
    "- For previous year questions, provide step-by-step solutions\n",
    "- If you need to make assumptions, state them clearly\n",
    "- If the context doesn't contain enough information, say so and suggest what additional materials might help\n",
    "\n",
    "Context from Study Materials:\n",
    "{context}\n",
    "\n",
    "Student Question: {question}\n",
    "\n",
    "Study Assistant Response:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the Study Assistant RAG chain\n",
    "if 'llm' in locals() and 'retriever' in locals():\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template}\n",
    "    )\n",
    "    \n",
    "    print(\"Software Engineering Study Assistant created successfully\")\n",
    "    print(\"Chain type: stuff (combines all retrieved study materials)\")\n",
    "    print(\"Returns source documents: Yes\")\n",
    "    print(\"Ready to help with your studies!\")\n",
    "else:\n",
    "    print(\"Cannot create Study Assistant - missing LLM or retriever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4746e52",
   "metadata": {},
   "source": [
    "## Test the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42d8e0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_8172\\993907196.py:11: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Question: What are the key principles of software design?\n",
      "\n",
      "Study Assistant Answer:\n",
      "\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: Unknown (Chunk 33)\n",
      "Content Preview: --- Lecture Page 7 ---\n",
      "\n",
      "Halstead’s Approach\n",
      "• Halstead’s Software Science is a theoretical approach to measuring software\n",
      "complexity and predicting at...\n",
      "\n",
      "Source 2: Unknown (Chunk 37)\n",
      "Content Preview: --- Lecture Page 14 ---\n",
      "\n",
      "Design Size\n",
      "• Object-oriented designs add new abstraction mechanisms: objects, classes, \n",
      "interfaces, operations, methods, ass...\n",
      "\n",
      "Source 3: Unknown (Chunk 26)\n",
      "Content Preview: ●​ Medium-sized projects with moderate complexity.​\n",
      " \n",
      "●​ Teams consist of both experienced and less-experienced members.​\n",
      " \n",
      "●​ Requirements may be par...\n",
      "\n",
      "Source 4: Unknown (Chunk 42)\n",
      "Content Preview: --- Lecture Page 25 ---\n",
      "\n",
      "Project Size - Metrics\n",
      "Availability of Size Estimation Metrics:\n",
      "Development Phase\n",
      "Available \n",
      "Metrics\n",
      "a\n",
      "Requirements Gathering...\n",
      "\n",
      "Source 5: Unknown (Chunk 25)\n",
      "Content Preview: 6. Estimate Project Metrics \n",
      " \n",
      "The COCOMO I (Constructive Cost Model I) is used to estimate the required development \n",
      "effort, time, team size, and cos...\n"
     ]
    }
   ],
   "source": [
    "def ask_study_question(question: str):\n",
    "    \"\"\"\n",
    "    Ask a study-related question using the RAG pipeline\n",
    "    \"\"\"\n",
    "    if 'rag_chain' not in locals() and 'rag_chain' not in globals():\n",
    "        print(\"Study Assistant not available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Get response from RAG chain\n",
    "        result = rag_chain({\"query\": question})\n",
    "        \n",
    "        print(f\"Student Question: {question}\")\n",
    "        print(f\"\\nStudy Assistant Answer:\\n{result['result']}\")\n",
    "        \n",
    "        # Show source materials referenced\n",
    "        print(f\"\\nSource Materials Referenced ({len(result['source_documents'])})\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, doc in enumerate(result['source_documents'], 1):\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "            chunk_id = doc.metadata.get('chunk_id', 'N/A')\n",
    "            print(f\"\\nSource {i}: {source} (Chunk {chunk_id})\")\n",
    "            print(f\"Content Preview: {doc.page_content[:150]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during study query: {e}\")\n",
    "\n",
    "# Test the Study Assistant with a sample question\n",
    "if 'rag_chain' in locals():\n",
    "    # Test with a typical software engineering question\n",
    "    ask_study_question(\"What are the key principles of software design?\")\n",
    "else:\n",
    "    print(\"Study Assistant not ready for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae045830",
   "metadata": {},
   "source": [
    "## Interactive Q&A Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1e01279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asking: What is the COCOMO model and how to estimate the cost of a software?\n",
      "============================================================\n",
      "Student Question: What is the COCOMO model and how to estimate the cost of a software?\n",
      "\n",
      "Study Assistant Answer:\n",
      "\n",
      "\n",
      "Source Materials Referenced (5)\n",
      "--------------------------------------------------\n",
      "\n",
      "Source 1: Unknown (Chunk 53)\n",
      "Content Preview: --- Lecture Page 49 ---\n",
      "\n",
      "Intermediate COCOMO Model\n",
      "• Intermediate COCOMO model is an extension of the Basic COCOMO \n",
      "model which includes a set of cost...\n",
      "\n",
      "Source 2: Sample.pdf (Chunk 57)\n",
      "Content Preview: --- Lecture Page 55 ---\n",
      "\n",
      "Detailed/Advanced COCOMO Model\n",
      "• In the Detailed COCOMO Model, the cost of each subsystem is estimated \n",
      "separately. This appr...\n",
      "\n",
      "Source 3: Unknown (Chunk 56)\n",
      "Content Preview: --- Lecture Page 54 ---\n",
      "\n",
      "Detailed/Advanced COCOMO Model\n",
      "• The model accounts for the influence of the \n",
      "individual development phase (analysis, \n",
      "design...\n",
      "\n",
      "Source 4: Unknown (Chunk 91)\n",
      "Content Preview: --- Lecture Page 17 ---\n",
      "\n",
      " \n",
      "To calculate the efficiency of the project, we use the following formula: \n",
      " \n",
      "Where: \n",
      "●​ Actual KLOC = 3.011 KLOC (the real ...\n",
      "\n",
      "Source 5: Unknown (Chunk 51)\n",
      "Content Preview: --- Lecture Page 46 ---\n",
      "\n",
      "Basic COCOMO Model\n",
      "• The first level, Basic COCOMO can be used for quick and slightly\n",
      "rough calculations of Software Costs.\n",
      "•...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ask your own study question here\n",
    "your_question = \"What is the COCOMO model and how to estimate the cost of a software?\"  # Modify this\n",
    "\n",
    "if 'rag_chain' in locals():\n",
    "    print(f\"\\nAsking: {your_question}\")\n",
    "    print(\"=\" * 60)\n",
    "    ask_study_question(your_question)\n",
    "else:\n",
    "    print(\"\\nStudy Assistant not ready. Please ensure all previous cells ran successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70297508",
   "metadata": {},
   "source": [
    "## Software Engineering Study Assistant Summary\n",
    "\n",
    "This notebook successfully implements a comprehensive study assistant for software engineering students with:\n",
    "\n",
    "1. **PyMuPDF** - Extracts text from lecture notes, textbooks, and previous year papers\n",
    "2. **RecursiveCharacterTextSplitter** - Creates intelligent chunks for better knowledge retrieval\n",
    "3. **Sentence Transformers** (all-MiniLM-L6-v2) - Semantic understanding of technical concepts\n",
    "4. **ChromaDB** - Fast search across your entire study material collection\n",
    "5. **LangChain Retriever** - Finds most relevant content for your questions\n",
    "6. **Gemini Pro** - Provides detailed explanations with proper context and references\n",
    "\n",
    "**Perfect for:**\n",
    "- Understanding complex software engineering concepts\n",
    "- Solving previous year exam questions step-by-step\n",
    "- Getting quick explanations with proper source references\n",
    "- Preparing for exams with comprehensive study assistance\n",
    "- Clarifying doubts from multiple lecture sources\n",
    "\n",
    "**To get started:**\n",
    "1. Get your free Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
    "2. Replace `your-gemini-api-key-here` with your actual API key\n",
    "3. Add your lecture notes PDFs to the `pdf_paths` list in the PDF extraction cell\n",
    "4. Run all cells to build your knowledge base\n",
    "5. Start asking questions about your course materials!\n",
    "\n",
    "**Pro Tips:**\n",
    "- Add all your course PDFs for comprehensive coverage\n",
    "- Ask specific questions for better answers\n",
    "- Use the source references to dive deeper into topics\n",
    "- Perfect for exam preparation and assignment help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
